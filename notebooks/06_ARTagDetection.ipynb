{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb3cf41",
   "metadata": {},
   "source": [
    "# **Task: Using the Drone's front camera, detect an AR tag, identify the Tag ID, then compute the drone's pose from the AR tag's information**\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Use a program to extract the AR tag and convert to a matrix to find the TAG ID\n",
    "2. Corrospond the AR tag to known location values in a pre-determined index\n",
    "3. Using the points of the AR tag, output the drone's pose in a 6 dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae0a61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb3e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import glob\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b0ff5",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "**Use a program to extract the AR tag to find the tag ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8162019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.0\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [87] on image: ARTags/id87.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [93] on image: ARTags/image5.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [95] on image: ARTags/id95.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [95] on image: ARTags/id95_2.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [95] on image: ARTags/image3.png\n",
      "<class 'NoneType'>\n",
      "No markers found for image: ARTags/image.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [93] on image: ARTags/image6.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [87] on image: ARTags/id87_2.png\n",
      "<class 'numpy.ndarray'>\n",
      "Detected ID: [93] on image: ARTags/id93_1.png\n",
      "<class 'NoneType'>\n",
      "No markers found for image: ARTags/id87_1.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cv2\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m imagepaths:\n\u001b[0;32m----> 7\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(path)\n\u001b[1;32m     10\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mconvertScaleAbs(gray, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folderpath = \"ARTags/\"\n",
    "imagepaths = glob.glob(os.path.join(folderpath, \"*.png\"))\n",
    "kernel = np.ones((3, 3),np.uint8)\n",
    "print(cv2.__version__)\n",
    "\n",
    "for path in imagepaths:\n",
    "    tag = cv2.imread(path)\n",
    "\n",
    "    \n",
    "    gray = cv2.cvtColor(tag, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    tag = cv2.convertScaleAbs(gray, alpha=1.0, beta=-50)\n",
    "    tag = cv2.erode(tag, kernel)\n",
    "\n",
    "\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_5X5_100)\n",
    "\n",
    "    detectorParams = aruco.DetectorParameters()\n",
    "    detectorParams.aprilTagCriticalRad = 0.1 #default 0.17\n",
    "    detectorParams.aprilTagMaxLineFitMse = 100 #default 10\n",
    "    detectorParams.aprilTagMaxNmaxima = 15 #default 10\n",
    "    detectorParams.polygonalApproxAccuracyRate = 0.01 #default 0.03\n",
    "    detectorParams.useAruco3Detection = True\n",
    "    detector = aruco.ArucoDetector(aruco_dict, detectorParams)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray)\n",
    "\n",
    "    if ids is not None: \n",
    "\n",
    "        print(f\"Detected ID: {ids.flatten()} on image: {path}\")\n",
    "        tag = aruco.drawDetectedMarkers(tag, corners, ids)\n",
    "    else:\n",
    "        print(f\"No markers found for image: {path}\")\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.imshow(image, cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a079c9c",
   "metadata": {},
   "source": [
    "# Step 2 \n",
    "**Corrospond the AR tag to a predetermined location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2226ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  0]]\n",
      "\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "T = np.array([[0, 0, 1, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 0, 0, 1]])\n",
    "\n",
    "print(T[:3, :3])\n",
    "print()\n",
    "print(T[:3, 3].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb6291",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "**Find the drone's position relative to the tag's position**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e657013",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER_LENGTH = 0.2667 #Meters\n",
    "CAMERA_MATRIX = np.array([])\n",
    "DIST_COEFFS = []\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "\n",
    "    sy = np.linalg.norm([R[0, 0], R[1, 0]])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = np.arctan2(R[2, 1], R[2, 2])  \n",
    "        y = np.arctan2(-R[2, 0], sy)      \n",
    "        z = np.arctan2(R[1, 0], R[0, 0])  \n",
    "    else:\n",
    "        x = np.arctan2(-R[1, 2], R[1, 1])\n",
    "        y = np.arctan2(-R[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "\n",
    "def func(corners):\n",
    "\n",
    "    rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(corners, MARKER_LENGTH, CAMERA_MATRIX, DIST_COEFFS)\n",
    "\n",
    "    R_marker_to_cam, _ = cv2.Rodrigues(rvecs)\n",
    "\n",
    "    R_cam_to_marker = R_marker_to_cam.T\n",
    "\n",
    "    t_cam_to_marker = -R_cam_to_marker @ tvecs #The position of the camera in the marker's coordinate frame\n",
    "\n",
    "    rvec_cam_to_marker, _ = cv2.Rodrigues(R_cam_to_marker) #The rotation of the camera in relation to the marker (in marker frame)\n",
    "\n",
    "    x, y, z = rotationMatrixToEulerAngles(R_cam_to_marker) #Roll, pitch, yaw\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451b9f1",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "\n",
    "**Write the ros node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "import cv2\n",
    "from std_msgs.msg import String\n",
    "from std_msgs.msg import Int32\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "import cv2.aruco as aruco\n",
    "import sys\n",
    "\n",
    "BASE_LOCATION = np.array([0, 0, 0])\n",
    "CAR_TAG_ID = 96\n",
    "CAR_MARKER_LENGTH = 0 #TBD -- Meters\n",
    "MARKER_LENGTH = 0.2667 #Meters\n",
    "CAMERA_MATRIX = np.array([])\n",
    "DIST_COEFFS = ()\n",
    "KERNEL = np.ones((5, 5),np.uint8)   \n",
    "T_FC_2_BD= np.array([[0, 0, 1, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 0, 0, 1]]) @ np.array(\n",
    "            [[1, 0, 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [0, 0, 0, 1]])\n",
    "T_MARKERS = []\n",
    "\n",
    "class ARTagDetector(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('artag_detector')\n",
    "        self.get_logger().info('ARTag Detector has been initialized')\n",
    "        \n",
    "        #Create camera subscription, 50 milli second wait\n",
    "        self.camera_sub = self.create_subscription(\n",
    "            Image,\n",
    "            '/world/line_following_track/model/x500_mono_cam_down_0/link/camera_link/sensor/imager/image',\n",
    "            self.camera_sub_cb,\n",
    "            50\n",
    "        )\n",
    "\n",
    "        self.tagid_pub = self.create_publisher(String, '/', 1)\n",
    "        self.position_pub = self.create_publisher(Int32, '/line/detector_image', 1)\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "    \"\"\"\n",
    "    camera callback\n",
    "    Publishes to tag id and position pub\n",
    "    \n",
    "    \"\"\"\n",
    "    def camera_sub_cb(self, msg):\n",
    "        # Convert Image msg to OpenCV image\n",
    "        image = self.bridge.imgmsg_to_cv2(msg, \"mono8\")\n",
    "\n",
    "        # Detect line in the image. detect returns a parameterize the line (if one exists)\n",
    "        tagids, corners = self.detect_id(image)\n",
    "        if tagids is None:\n",
    "            self.get_logger().info('No ARTag found!')\n",
    "        else:\n",
    "            self.get_logger().info(f'ARTag Detected! Detected: {tagids.flatten()}')\n",
    "\n",
    "\n",
    "\n",
    "        # Aruco Estimate pose\n",
    "        rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(corners, MARKER_LENGTH, CAMERA_MATRIX, DIST_COEFFS)\n",
    "\n",
    "        worldPose = self.calculateDronePose(rvecs,tvecs,tagids)\n",
    "\n",
    "\n",
    "\n",
    "    def calculateDronePose(self, rvecs, tvecs, tagids):\n",
    "        #Rotate to Euler angles\n",
    "        def rotationMatrixToEulerAngles(R):\n",
    "            sy = np.linalg.norm([R[0, 0], R[1, 0]])\n",
    "            singular = sy < 1e-6\n",
    "\n",
    "            if not singular:\n",
    "                x = np.arctan2(R[2, 1], R[2, 2])  \n",
    "                y = np.arctan2(-R[2, 0], sy)      \n",
    "                z = np.arctan2(R[1, 0], R[0, 0])  \n",
    "            else:\n",
    "                x = np.arctan2(-R[1, 2], R[1, 1])\n",
    "                y = np.arctan2(-R[2, 0], sy)\n",
    "                z = 0\n",
    "            return np.array([x, y, z])\n",
    "\n",
    "        posTags = {}\n",
    "        \n",
    "        # Processing each tag...\n",
    "        for i in range(len(tagids)):\n",
    "            T = np.eye(4)\n",
    "            marker_id = tagids[i]\n",
    "            T_marker = T_MARKERS[marker_id]\n",
    "\n",
    "            rvec = rvecs[i]\n",
    "            tvec = tvecs[i]\n",
    "\n",
    "            #Turn Rotation to world frame\n",
    "            R_marker_to_cam, _ = cv2.Rodrigues(rvec)\n",
    "            R_cam_to_marker = R_marker_to_cam.T\n",
    "            #The rotation of the camera in relation to the marker (in marker frame)\n",
    "            rvec_cam_to_marker, _ = cv2.Rodrigues(R_cam_to_marker) \n",
    "            roll, pitch, yaw = rotationMatrixToEulerAngles(rvec_cam_to_marker)     #Roll, pitch, yaw\n",
    "\n",
    "            #Turn translation to world frame\n",
    "            T[:3, :3] = R_marker_to_cam\n",
    "            T[:3, 3] = tvec.flatten()\n",
    "            T_fc_in_marker = np.linalg.inv(T) \n",
    "            T_fc_in_lenu = T_marker @ T_fc_in_marker \n",
    "            #The position of the camera in the marker's coordinate frame (NOT RELATIVE TO THE MARKER)\n",
    "            T_bd_in_lenu = T_fc_in_lenu @ T_FC_2_BD\n",
    "\n",
    "            posTags[marker_id] = T_bd_in_lenu[:3, 3].T \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Function Description: Passing in a grayscale cv2 image and detects the AR tag based on the aruco 5x5 dictionary\n",
    "    with numbers 1-100 (WARNING: IF TAG NOT FULLY VISIBLE AS SQUARE WILL NOT WORK)\n",
    "\n",
    "    Input: Cv2 Image\n",
    "    Output: numpy array of TAGIDs (may be multiple)\n",
    "    \"\"\"\n",
    "    def detect_id(self, image):\n",
    "        image = cv2.convertScaleAbs(gray, alpha=1.0, beta=-50)\n",
    "        image = cv2.erode(image, kernel)\n",
    "\n",
    "\n",
    "        aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_5X5_100)\n",
    "\n",
    "        detectorParams = aruco.DetectorParameters()\n",
    "        detectorParams.aprilTagCriticalRad = 0.1 #default 0.17\n",
    "        detectorParams.aprilTagMaxLineFitMse = 100 #default 10\n",
    "        detectorParams.aprilTagMaxNmaxima = 15 #default 10\n",
    "        detectorParams.polygonalApproxAccuracyRate = 0.01 #default 0.03\n",
    "        detectorParams.useAruco3Detection = True\n",
    "        detector = aruco.ArucoDetector(aruco_dict, detectorParams)\n",
    "        corners, ids, rejected = detector.detectMarkers(gray)\n",
    "\n",
    "        return ids.flatten(), corners\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    detector = ARTagDetector()\n",
    "    detector.get_logger.info('ARTag detector initialized')\n",
    "    try:\n",
    "        rclpy.spin(detector)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"ARTag detector shutting down\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        detector.destroy_node()\n",
    "        rclpy.shutdown()\n",
    "\n",
    "\n",
    "if __name__=='__main__':#Unneeded if you only ever run the node directly with ros2 run \n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc41798",
   "metadata": {},
   "source": [
    "Making separate ros node for ar tag for car..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import os\n",
    "import glob\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "\n",
    "IDS = []\n",
    "CURR_DIST = [0.0, 0.0, 0.0]\n",
    "MARKER_SIZE = 26.6 #Centimeters\n",
    "ARUCODICT = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_5X5_100)\n",
    "IN_MATRIX = np.array([[1.29650191e+03, 0.00000000e+00, 4.14079631e+02],#intrinsic camera matrix, calibrate with\n",
    "                    [0.00000000e+00, 1.29687850e+03, 2.26798449e+02],#https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
    "                    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
    "DISTORTION = np.array([[2.56660190e-01, -2.23374068e+00, -2.54856563e-03,  4.09905103e-03,9.68094572e+00]])\n",
    "MARKER_POINTS = np.array([[-MARKER_SIZE / 2, MARKER_SIZE / 2, 0],#corners of AR in world frame\n",
    "                [MARKER_SIZE / 2, MARKER_SIZE / 2, 0],\n",
    "                [MARKER_SIZE / 2, -MARKER_SIZE / 2, 0],\n",
    "                [-MARKER_SIZE / 2, -MARKER_SIZE / 2, 0]], dtype=np.float32)\n",
    "\n",
    "    \n",
    "def rodrigues_to_euler(rvec):\n",
    "    # Convert Rodrigues vector to rotation matrix\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rvec)\n",
    "    # Check for gimbal lock\n",
    "    sy = np.sqrt(rotation_matrix[0, 0] ** 2 + rotation_matrix[1, 0] ** 2)\n",
    "    locked = sy < 1e-6\n",
    "\n",
    "    if not locked:\n",
    "        roll = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "        pitch = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        yaw = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    else:\n",
    "        roll = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "        pitch = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        yaw = 0\n",
    "\n",
    "    return np.array([roll, pitch, yaw])\n",
    "\n",
    "        \n",
    "def grab_tags(tag):\n",
    "    global IDS\n",
    "    # Preprocess\n",
    "    if tag is None:\n",
    "        print(\"Error: Image is None.\")\n",
    "        return []\n",
    "    tag = cv2.cvtColor(tag, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    tag = cv2.convertScaleAbs(gray, alpha=1.0, beta=-50)\n",
    "    tag = cv2.erode(tag, kernel)\n",
    "\n",
    "\n",
    "    detectorParams = aruco.DetectorParameters()\n",
    "    detectorParams.aprilTagCriticalRad = 0.1 #default 0.17\n",
    "    detectorParams.aprilTagMaxLineFitMse = 100 #default 10\n",
    "    detectorParams.aprilTagMaxNmaxima = 15 #default 10\n",
    "    detectorParams.polygonalApproxAccuracyRate = 0.01 #default 0.03\n",
    "    detectorParams.useAruco3Detection = True\n",
    "    detector = aruco.ArucoDetector(ARUCODICT, detectorParams)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray)\n",
    "    IDS = ids\n",
    "\n",
    "    # rvec, tvec = my_estimatePoseSingleMarkers2(corners, mtx, distortion)#get stats from AR tag\n",
    "    #rvec=rodrigues vector, tvec=translation vector of the AR tag in the camera frame\n",
    "    rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(corners, MARKER_SIZE, IN_MATRIX, DISTORTION)\n",
    "\n",
    "    # cv.drawFrameAxes(tag, mtx, distortion, rvec, tvec, length=10)#optional, helps visualization\n",
    "    #red=x, y=green, z=blue\n",
    "    # print(f\"rvecs: {rvec}, tvecs: {tvec}\\n\")\n",
    "    return rvecs, tvecs\n",
    "\n",
    "def find_pos(rvec1, tvec1):\n",
    "    rot_mat, jacobian = cv2.Rodrigues(rvec1)\n",
    "    wRd = np.transpose(rot_mat)#turns drone frame to world frame\n",
    "    drone_from_ar = np.matmul(wRd, -tvec1)#finds translation vector from tag to drone in world frame\n",
    "    #negative because tvec1 is from camera to tag, we want from tag to camera\n",
    "    orien = rodrigues_to_euler(rvec1)\n",
    "    return drone_from_ar, orien\n",
    "\n",
    "\n",
    "\n",
    "def find_relative_pose(pic):\n",
    "    #Each AR tag has a number written on it and a unique color combo, this will contain info for each \n",
    "    result = grab_tags(pic, ARUCODICT, IN_MATRIX, DISTORTION)#find and process the image\n",
    "    maxDist = 0\n",
    "    maxTrans,  maxOrien, id = None, None, None\n",
    "    for index, res in enumerate(result):\n",
    "        rvec, tvec = res\n",
    "        tvec_m = tvec*0.01#convert to meters, the 26.6 market size was in cm\n",
    "        trans, orien = find_pos(rvec, tvec_m)#get position relative to AR tag\n",
    "        d = math.dist(trans, CURR_DIST)\n",
    "        if d < maxDist:\n",
    "            maxTrans = trans\n",
    "            maxDist = d\n",
    "            maxOrien = orien\n",
    "            id = IDS[index]\n",
    "    \n",
    "    return maxTrans, maxOrien, \n",
    "        \n",
    "\n",
    "path = \"PICAM_ARTAGS/\"\n",
    "image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "if image is None:\n",
    "    print(\"Failed to load image. Check the path.\")\n",
    "\n",
    "trans, orien = find_relative_pose(image)\n",
    "print(f\"Relative distance from tag: {trans} and orientation according to tag: {orien}\")\n",
    "cv2.imshow(\"Pose Debug\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f681e3d",
   "metadata": {},
   "source": [
    "Making a camera feed processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "LOW = np.array([250, 250, 250])  # Lower image thresholding bound\n",
    "HI = np.array([255, 255, 255])   # Upper image thresholding bound\n",
    "\n",
    "KERNEL_D = np.ones((30, 30), np.uint8)\n",
    "KERNEL_E = np.ones((20, 20), np.uint8)\n",
    "\n",
    "filename = \"easdifj\"\n",
    "\n",
    "vidpath = f\"cameravids/{filename}.h64\"\n",
    "outpath = f\"cameravids/{filename}_processed.mp4\"\n",
    "interval = 0.1\n",
    "\n",
    "cap = cv2.VideoCapture(vidpath)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_interval = int(fps * interval)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can also use 'XVID'\n",
    "out = cv2.VideoWriter(outpath, fourcc, fps, (width, height))\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Only process every nth frame\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # (Optional) Do processing here\n",
    "        image = frame\n",
    "        image = cv2.dilate(image, KERNEL_D, iterations = 1)\n",
    "        image = cv2.erode(image, KERNEL_E, iterations = 1)\n",
    "        mask = cv2.inRange(image, LOW, HI)\n",
    "        image = cv2.bitwise_and(image, image, mask=mask)\n",
    "        image = cv2.GaussianBlur(image, (5,5), 0)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, image = cv2.threshold(image,245,255,cv2.THRESH_BINARY)\n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt_sort = lambda cnt: (max(cv2.minAreaRect(cnt)[1]))\n",
    "        sorted_contours = sorted(contours, key=cnt_sort, reverse=True)\n",
    "\n",
    "        if len(sorted_contours) == 0:\n",
    "            cv2.putText(image, \"No line detected!\", (image.shape[0]//2, image.shape[1]//2), color=(0, 0, 255), thickness=2)\n",
    "\n",
    "        else:\n",
    "            all_points = np.vstack(sorted_contours[0])\n",
    "            [vx, vy, x, y] = cv2.fitLine(all_points, cv2.DIST_L2, 0, 0.01, 0.01)\n",
    "            x, y, vx, vy = x.item(), y.item(), vx.item(), vy.item()\n",
    "\n",
    "            # rows, cols = image.shape\n",
    "            left_y = int((-x * vy / vx) + y)\n",
    "            right_y = int(((cols - x) * vy / vx) + y)\n",
    "            cv2.line(image, (0, left_y), (image.shape[1] - 1, right_y), (0, 255, 0), 2)\n",
    "            \n",
    "        out.write(image)  # Save to output video\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Success! Video saved\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
